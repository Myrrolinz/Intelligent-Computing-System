{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd1db65-a4d5-48f6-94c1-de13374da2be",
   "metadata": {},
   "source": [
    "## Caffe2Darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c8492-34e1-49eb-aea7-1649d36ef6cd",
   "metadata": {},
   "source": [
    "执行Caffe2Darknet的模型转换操作，可选择使用 Caffe2Darknet 模型转换工具 或其他开源的模型转换工具。可根据任务二的要求，发现 Caffe2Darknet 模型转换工具现存的缺陷，对代码进行改进，以期其能进行正确的模型转换和推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d612389-16a2-4c24-9577-c55229332b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "\n",
    "import caffe.proto.caffe_pb2 as caffe_pb2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0786d68-d4b2-4d12-8247-8add360ea77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _save_weights(data, weightfile):\n",
    "    \"\"\"\n",
    "    将权重数据数组data保存到指定的文件weightfile中\n",
    "    \"\"\"\n",
    "    print('Save to ', weightfile)\n",
    "    wsize = data.size\n",
    "    weights = np.zeros((wsize + 4, ), dtype=np.int32)\n",
    "    weights[0] = 0\n",
    "    weights[1] = 1\n",
    "    weights[2] = 0\n",
    "    weights[3] = 0\n",
    "    weights.tofile(weightfile)\n",
    "    weights = np.fromfile(weightfile, dtype=np.float32)\n",
    "    weights[4:] = data\n",
    "    weights.tofile(weightfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1db15c3-d4f1-4c8b-aaa6-6003ce897f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _save_cfg(blocks, cfgfile):\n",
    "    \"\"\"\n",
    "    将神经网络配置块保存到文件\n",
    "    \"\"\"\n",
    "    with open(cfgfile, 'w') as fp:\n",
    "        for block in blocks:\n",
    "            fp.write('[%s]\\n' % (block['type']))\n",
    "            for key, value in block.items():\n",
    "                if key != 'type':\n",
    "                    fp.write(f'{key}={value}\\n')\n",
    "            fp.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca40ed57-f3f8-4b64-9423-dcb976228302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _print_cfg(blocks):\n",
    "    \"\"\"\n",
    "    打印Darknet框架中的神经网络配置块\n",
    "    \"\"\"\n",
    "    for block in blocks:\n",
    "        print('[%s]' % (block['type']))\n",
    "        for key, value in block.items():\n",
    "            if key != 'type':\n",
    "                print(f'{key}={value}')\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de9b2f3-a991-4d8c-a1b3-164d25ade652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_cfg_nicely(blocks):\n",
    "    print('layer     filters    size              input                output')\n",
    "    \"\"\"\n",
    "    prev_width：上一层网络的输出宽度\n",
    "    prev_height：上一层网络的输出高度\n",
    "    prev_filters：上一层网络的通道数\n",
    "    \"\"\"\n",
    "    prev_width = 416\n",
    "    prev_height = 416\n",
    "    prev_filters = 3\n",
    "    out_filters = []\n",
    "    out_widths = []\n",
    "    out_heights = []\n",
    "    ind = -2\n",
    "    for block in blocks:\n",
    "        ind = ind + 1\n",
    "        \n",
    "        # HERE! 感觉这里需要加一下block是flatten的操作\n",
    "        # 因为caffe modle最后的操作有：avgpool->flatten->fc，但是这里并没有实现flatten相关\n",
    "        # 在pytorch中（更好理解）的语句是x=x.flatten(1)\n",
    "        # flatten的作用是Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡\n",
    "        # 我看的flatten参考资料在这里~：https://blog.csdn.net/kuan__/article/details/116987162#%E6%80%BB%E7%BB%93\n",
    "            \n",
    "        if block['type'] == 'net':\n",
    "            prev_width = int(block['width'])\n",
    "            prev_height = int(block['height'])\n",
    "            continue\n",
    "\n",
    "        elif block['type'] == 'convolutional':\n",
    "            filters = int(block['filters'])\n",
    "            kernel_size = int(block['size'])\n",
    "            stride = int(block['stride'])\n",
    "            is_pad = int(block['pad'])\n",
    "            pad = (kernel_size - 1) / 2 if is_pad else 0\n",
    "            width = (prev_width + 2 * pad - kernel_size) / stride + 1\n",
    "            height = (prev_height + 2 * pad - kernel_size) / stride + 1\n",
    "            print('%5d %-6s %4d  %d x %d / %d   %3d x %3d \\\n",
    "                x%4d   ->   %3d x %3d x%4d' %\n",
    "                  (ind, 'conv', filters, kernel_size, kernel_size, stride,\n",
    "                   prev_width, prev_height, prev_filters, width, height,\n",
    "                   filters))\n",
    "            prev_width = width\n",
    "            prev_height = height\n",
    "            prev_filters = filters\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "            \n",
    "        #在遍历网络结构时，在处理完上一层输出的宽、高和通道数之后，将 prev_width、prev_height 和 prev_filters \n",
    "        #相乘，就能计算出展开后的向量长度了。     \n",
    "        elif block['type'] == 'flatten':\n",
    "            #prev_width = int(block['width'])\n",
    "            #prev_height = int(block['height'])\n",
    "            #prev_filters = int(block['filters'])  \n",
    "            flat_size = prev_width * prev_height * prev_filters\n",
    "            print('%5d %-6s %10d' % (ind, 'flat', flat_size))\n",
    "            prev_width = flat_size\n",
    "            prev_height = 1\n",
    "            prev_filters = 1\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "            \n",
    "        elif block['type'] == 'maxpool':\n",
    "            pool_size = int(block['size'])\n",
    "            stride = int(block['stride'])\n",
    "            width = prev_width / stride\n",
    "            height = prev_height / stride\n",
    "            print('%5d %-6s       %d x %d / %d   %3d \\\n",
    "                x %3d x%4d   ->   %3d x %3d x%4d' %\n",
    "                  (ind, 'max', pool_size, pool_size, stride, prev_width,\n",
    "                   prev_height, prev_filters, width, height, filters))\n",
    "            prev_width = width\n",
    "            prev_height = height\n",
    "            prev_filters = filters\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'avgpool':\n",
    "            width = 1\n",
    "            height = 1\n",
    "            print('%5d %-6s                   %3d x %3d x%4d   ->      %3d' %\n",
    "                  (ind, 'avg', prev_width, prev_height, prev_filters,\n",
    "                   prev_filters))\n",
    "            prev_width = 1\n",
    "            prev_height = 1\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'softmax':\n",
    "            print('%5d %-6s                                    ->      %3d' %\n",
    "                  (ind, 'softmax', prev_filters))\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'cost':\n",
    "            print('%5d %-6s                                     ->      %3d' %\n",
    "                  (ind, 'cost', prev_filters))\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'reorg':\n",
    "            stride = int(block['stride'])\n",
    "            filters = stride * stride * prev_filters\n",
    "            width = prev_width / stride\n",
    "            height = prev_height / stride\n",
    "            print('%5d %-6s             / %d   %3d x %3d \\\n",
    "                x%4d   ->   %3d x %3d x%4d' %\n",
    "                  (ind, 'reorg', stride, prev_width, prev_height, prev_filters,\n",
    "                   width, height, filters))\n",
    "            prev_width = width\n",
    "            prev_height = height\n",
    "            prev_filters = filters\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'route':\n",
    "            layers = block['layers'].split(',')\n",
    "            layers = [int(i) if int(i) > 0 else int(i) + ind for i in layers]\n",
    "            if len(layers) == 1:\n",
    "                print('%5d %-6s %d' % (ind, 'route', layers[0]))\n",
    "                prev_width = out_widths[layers[0]]\n",
    "                prev_height = out_heights[layers[0]]\n",
    "                prev_filters = out_filters[layers[0]]\n",
    "            elif len(layers) == 2:\n",
    "                print('%5d %-6s %d %d' % (ind, 'route', layers[0], layers[1]))\n",
    "                prev_width = out_widths[layers[0]]\n",
    "                prev_height = out_heights[layers[0]]\n",
    "                assert (prev_width == out_widths[layers[1]])\n",
    "                assert (prev_height == out_heights[layers[1]])\n",
    "                prev_filters = out_filters[layers[0]] + out_filters[layers[1]]\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'region':\n",
    "            print('%5d %-6s' % (ind, 'detection'))\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'shortcut':\n",
    "            from_id = int(block['from'])\n",
    "            from_id = from_id if from_id > 0 else from_id + ind\n",
    "            print('%5d %-6s %d' % (ind, 'shortcut', from_id))\n",
    "            prev_width = out_widths[from_id]\n",
    "            prev_height = out_heights[from_id]\n",
    "            prev_filters = out_filters[from_id]\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'softmax':\n",
    "            print('%5d %-6s' % (ind, 'softmax'))\n",
    "            out_widths.append(prev_width)\n",
    "            out_heights.append(prev_height)\n",
    "            out_filters.append(prev_filters)\n",
    "        elif block['type'] == 'connected':\n",
    "            filters = int(block['output'])\n",
    "            print('%5d %-6s                            %d  ->      %3d' %\n",
    "                  (ind, 'connected', prev_filters, filters))\n",
    "            prev_filters = filters\n",
    "            out_widths.append(1)\n",
    "            out_heights.append(1)\n",
    "            out_filters.append(prev_filters)\n",
    "        else:\n",
    "            print('unknown type %s' % (block['type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f25486-ff15-4dcc-82ac-b8272fa0ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_caffemodel(caffemodel):\n",
    "    model = caffe_pb2.NetParameter()\n",
    "    print('Loading caffemodel: ', caffemodel)\n",
    "    with open(caffemodel, 'rb') as fp:\n",
    "        model.ParseFromString(fp.read())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de6443d-2ec3-43c3-8c65-38214d75f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_prototxt(protofile):\n",
    "\n",
    "    def line_type(line):\n",
    "        if line.find(':') >= 0:\n",
    "            return 0\n",
    "        elif line.find('{') >= 0:\n",
    "            return 1\n",
    "        return -1\n",
    "\n",
    "    def parse_block(fp):\n",
    "        block = OrderedDict()\n",
    "        line = fp.readline().strip()\n",
    "        while line != '}':\n",
    "            ltype = line_type(line)\n",
    "            if ltype == 0:  # key: value\n",
    "                # print line\n",
    "                line = line.split('#')[0]\n",
    "                key, value = line.split(':')\n",
    "                key = key.strip()\n",
    "                value = value.strip().strip('\"')\n",
    "                if key in block:\n",
    "                    if type(block[key]) == list:\n",
    "                        block[key].append(value)\n",
    "                    else:\n",
    "                        block[key] = [block[key], value]\n",
    "                else:\n",
    "                    block[key] = value\n",
    "            elif ltype == 1:  # blockname {\n",
    "                key = line.split('{')[0].strip()\n",
    "                sub_block = parse_block(fp)\n",
    "                block[key] = sub_block\n",
    "            line = fp.readline().strip()\n",
    "            line = line.split('#')[0]\n",
    "        return block\n",
    "    fp = open(protofile)\n",
    "    props = OrderedDict()\n",
    "    layers = []\n",
    "    line = fp.readline()\n",
    "    while line != '':\n",
    "        line = line.strip().split('#')[0]\n",
    "        if line == '':\n",
    "            line = fp.readline()\n",
    "            continue\n",
    "        ltype = line_type(line)\n",
    "        if ltype == 0:  # key: value\n",
    "            key, value = line.split(':')\n",
    "            key = key.strip()\n",
    "            value = value.strip().strip('\"')\n",
    "            if key in props:\n",
    "                if type(props[key]) == list:\n",
    "                    props[key].append(value)\n",
    "                else:\n",
    "                    props[key] = [props[key], value]\n",
    "            else:\n",
    "                props[key] = value\n",
    "        elif ltype == 1:  # blockname {\n",
    "            key = line.split('{')[0].strip()\n",
    "            if key == 'layer':\n",
    "                layer = parse_block(fp)\n",
    "                layers.append(layer)\n",
    "            else:\n",
    "                props[key] = parse_block(fp)\n",
    "        line = fp.readline()\n",
    "\n",
    "    if len(layers) > 0:\n",
    "        net_info = OrderedDict()\n",
    "        net_info['props'] = props\n",
    "        net_info['layers'] = layers\n",
    "        return net_info\n",
    "    else:\n",
    "        return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb606d0d-4ce4-4bf7-a43f-b811c812d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_hash(weightfile, cfgfile):\n",
    "    with open(weightfile, 'rb') as file:\n",
    "        md5 = hashlib.md5()\n",
    "        while True:\n",
    "            chunk = file.read(4096)\n",
    "            if not chunk:\n",
    "                break\n",
    "            md5.update(chunk)\n",
    "    with open(cfgfile, 'rb') as file:\n",
    "        md5 = hashlib.md5()\n",
    "        while True:\n",
    "            chunk = file.read(4096)\n",
    "            if not chunk:\n",
    "                break\n",
    "            md5.update(chunk)\n",
    "    hash_str = md5.hexdigest()\n",
    "    return hash_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f21cb167-4986-4abe-9561-4a852438c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_hash(weightfile, cfgfile, hash_str):\n",
    "    tmp_hash_str = _generate_hash(weightfile, cfgfile)\n",
    "    print(tmp_hash_str == hash_str)\n",
    "    assert tmp_hash_str == hash_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ab75be6-9b8a-41f9-9db2-da15092c369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caffe2Darknet:\n",
    "\n",
    "    def get_attribute(self, n):\n",
    "        if n in self.__dict__:\n",
    "            return self.__dict__[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for name, value in kwargs.items():\n",
    "            setattr(self, name, value)\n",
    "        if self.get_attribute('net') is not None:\n",
    "            assert isinstance(self.get_attribute('net'), str)\n",
    "            self.net = self.get_attribute('net')\n",
    "        else:\n",
    "            self.net = None\n",
    "        if self.get_attribute('weight') is not None:\n",
    "            assert isinstance(self.get_attribute('weight'), str)\n",
    "            self.weight = self.get_attribute('weight')\n",
    "        else:\n",
    "            self.weight = None\n",
    "        if self.get_attribute('out_file') is not None:\n",
    "            assert isinstance(self.get_attribute('out_file'), str)\n",
    "            self.out_file = self.get_attribute('out_file')\n",
    "        else:\n",
    "            self.out_file = None\n",
    "\n",
    "    def convert(self):\n",
    "        protofile = self.net\n",
    "        caffemodel = self.weight\n",
    "        model = _parse_caffemodel(caffemodel)\n",
    "        layers = model.layer\n",
    "        if len(layers) == 0:\n",
    "            print('Using V1LayerParameter')\n",
    "            layers = model.layers\n",
    "\n",
    "        lmap = {}\n",
    "        for l_name in layers:\n",
    "            lmap[l_name.name] = l_name\n",
    "\n",
    "        net_info = _parse_prototxt(protofile)\n",
    "        props = net_info['props']\n",
    "\n",
    "        wdata = []\n",
    "        blocks = []\n",
    "        block = OrderedDict()\n",
    "        block['type'] = 'net'\n",
    "        if 'input_shape' in props:\n",
    "            block['batch'] = props['input_shape']['dim'][0]\n",
    "            block['channels'] = props['input_shape']['dim'][1]\n",
    "            block['height'] = props['input_shape']['dim'][2]\n",
    "            block['width'] = props['input_shape']['dim'][3]\n",
    "        else:\n",
    "            block['batch'] = props['input_dim'][0]\n",
    "            block['channels'] = props['input_dim'][1]\n",
    "            block['height'] = props['input_dim'][2]\n",
    "            block['width'] = props['input_dim'][3]\n",
    "        if 'mean_file' in props:\n",
    "            block['mean_file'] = props['mean_file']\n",
    "        blocks.append(block)\n",
    "\n",
    "        layers = net_info['layers']\n",
    "        layer_num = len(layers)\n",
    "        i = 0  # layer id\n",
    "        layer_id = dict()\n",
    "        layer_id[props['input']] = 0\n",
    "        while i < layer_num:\n",
    "            layer = layers[i]\n",
    "            print(i, layer['name'], layer['type'])\n",
    "            if layer['type'] == 'Convolution':\n",
    "                if layer_id[layer['bottom']] != len(blocks) - 1:\n",
    "                    block = OrderedDict()\n",
    "                    block['type'] = 'route'\n",
    "                    block['layers'] = \\\n",
    "                        str(layer_id[layer['bottom']] - len(blocks))\n",
    "                    blocks.append(block)\n",
    "                # assert(i+1 < layer_num and\n",
    "                #   layers[i+1]['type'] == 'BatchNorm')\n",
    "                # assert(i+2 < layer_num and\n",
    "                #   layers[i+2]['type'] == 'Scale')\n",
    "                conv_layer = layers[i]\n",
    "                block = OrderedDict()\n",
    "                block['type'] = 'convolutional'\n",
    "                block['filters'] = conv_layer['convolution_param'][\n",
    "                    'num_output']\n",
    "                block['size'] = conv_layer['convolution_param']['kernel_size']\n",
    "                block['stride'] = conv_layer['convolution_param']['stride']\n",
    "                block['pad'] = '1'\n",
    "                last_layer = conv_layer\n",
    "                m_conv_layer = lmap[conv_layer['name']]\n",
    "                if i + 2 < layer_num and layers[\n",
    "                        i + 1]['type'] == 'BatchNorm' and layers[\n",
    "                            i + 2]['type'] == 'Scale':\n",
    "                    print(i + 1, layers[i + 1]['name'], layers[i + 1]['type'])\n",
    "                    print(i + 2, layers[i + 2]['name'], layers[i + 2]['type'])\n",
    "                    block['batch_normalize'] = '1'\n",
    "                    bn_layer = layers[i + 1]\n",
    "                    scale_layer = layers[i + 2]\n",
    "                    last_layer = scale_layer\n",
    "                    m_scale_layer = lmap[scale_layer['name']]\n",
    "                    m_bn_layer = lmap[bn_layer['name']]\n",
    "                    wdata += list(m_scale_layer.blobs[1].data)\n",
    "                    wdata += list(m_scale_layer.blobs[0].data)\n",
    "                    wdata += (np.array(m_bn_layer.blobs[0].data) /\n",
    "                              m_bn_layer.blobs[2].data[0]).tolist()\n",
    "                    wdata += (np.array(m_bn_layer.blobs[1].data) /\n",
    "                              m_bn_layer.blobs[2].data[0]).tolist()\n",
    "                    i = i + 2\n",
    "                else:\n",
    "                    wdata += list(m_conv_layer.blobs[1].data)\n",
    "                wdata += list(m_conv_layer.blobs[0].data)\n",
    "\n",
    "                if i + 1 < layer_num and layers[i + 1]['type'] == 'ReLU':\n",
    "                    print(i + 1, layers[i + 1]['name'], layers[i + 1]['type'])\n",
    "                    act_layer = layers[i + 1]\n",
    "                    block['activation'] = 'relu'\n",
    "                    top = act_layer['top']\n",
    "                    layer_id[top] = len(blocks)\n",
    "                    blocks.append(block)\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    block['activation'] = 'linear'\n",
    "                    top = last_layer['top']\n",
    "                    layer_id[top] = len(blocks)\n",
    "                    blocks.append(block)\n",
    "                i = i + 1\n",
    "            elif layer['type'] == 'Pooling':\n",
    "                assert (layer_id[layer['bottom']] == len(blocks) - 1)\n",
    "                block = OrderedDict()\n",
    "                if layer['pooling_param']['pool'] == 'AVE':\n",
    "                    block['type'] = 'avgpool'\n",
    "                elif layer['pooling_param']['pool'] == 'MAX':\n",
    "                    block['type'] = 'maxpool'\n",
    "                    block['size'] = layer['pooling_param']['kernel_size']\n",
    "                    block['stride'] = layer['pooling_param']['stride']\n",
    "                    if 'pad' in layer['pooling_param']:\n",
    "                        pad = int(layer['pooling_param']['pad'])\n",
    "                        if pad > 0:\n",
    "                            block['pad'] = '1'\n",
    "                top = layer['top']\n",
    "                layer_id[top] = len(blocks)\n",
    "                blocks.append(block)\n",
    "                i = i + 1\n",
    "                \n",
    "            #HERE!在这里加了一下flatten的处理过程！\n",
    "            elif layer['type'] == 'Flatten':\n",
    "                assert (layer_id[layer['bottom']] == len(blocks) - 1)\n",
    "                block = OrderedDict()\n",
    "                #block['type'] = 'flatten'\n",
    "                block['type'] = 'connected'\n",
    "                block['output'] = 512\n",
    "                block['activation'] = 'linear'\n",
    "                top = layer['top']\n",
    "                layer_id[top] = len(blocks)\n",
    "                blocks.append(block)\n",
    "                i = i + 1\n",
    "                # i = i +1 \n",
    "                # continue\n",
    "                # 处理 flatten 层\n",
    "                #flatten_size = int(prev_width * prev_height * prev_channels)\n",
    "                #prev_width, prev_height, prev_channels = 1, 1, flatten_size\n",
    "            elif layer['type'] == 'Eltwise':\n",
    "                bottoms = layer['bottom']\n",
    "                bottom1 = layer_id[bottoms[0]] - len(blocks)\n",
    "                bottom2 = layer_id[bottoms[1]] - len(blocks)\n",
    "                assert (bottom1 == -1 or bottom2 == -1)\n",
    "                from_id = bottom2 if bottom1 == -1 else bottom1\n",
    "                block = OrderedDict()\n",
    "                block['type'] = 'shortcut'\n",
    "                block['from'] = str(from_id)\n",
    "                assert (i + 1 < layer_num and layers[i + 1]['type'] == 'ReLU')\n",
    "                block['activation'] = 'relu'\n",
    "                top = layers[i + 1]['top']\n",
    "                layer_id[top] = len(blocks)\n",
    "                blocks.append(block)\n",
    "                i = i + 2\n",
    "            elif layer['type'] == 'InnerProduct':\n",
    "                assert (layer_id[layer['bottom']] == len(blocks) - 1)\n",
    "                block = OrderedDict()\n",
    "                block['type'] = 'connected'\n",
    "                block['output'] = layer['inner_product_param']['num_output']\n",
    "                m_fc_layer = lmap[layer['name']]\n",
    "                wdata += list(m_fc_layer.blobs[1].data)\n",
    "                wdata += list(m_fc_layer.blobs[0].data)\n",
    "                if i + 1 < layer_num and layers[i + 1]['type'] == 'ReLU':\n",
    "                    act_layer = layers[i + 1]\n",
    "                    block['activation'] = 'relu'\n",
    "                    top = act_layer['top']\n",
    "                    layer_id[top] = len(blocks)\n",
    "                    blocks.append(block)\n",
    "                    i = i + 2\n",
    "                else:\n",
    "                    block['activation'] = 'linear'\n",
    "                    top = layer['top']\n",
    "                    layer_id[top] = len(blocks)\n",
    "                    blocks.append(block)\n",
    "                    i = i + 1\n",
    "            elif layer['type'] == 'Softmax':\n",
    "                assert (layer_id[layer['bottom']] == len(blocks) - 1)\n",
    "                block = OrderedDict()\n",
    "                block['type'] = 'softmax'\n",
    "                block['groups'] = 1\n",
    "                top = layer['top']\n",
    "                layer_id[top] = len(blocks)\n",
    "                blocks.append(block)\n",
    "                i = i + 1\n",
    "            else:\n",
    "                print('unknown type %s' % layer['type'])\n",
    "                if layer_id[layer['bottom']] != len(blocks) - 1:\n",
    "                    block = OrderedDict()\n",
    "                    block['type'] = 'route'\n",
    "                    block['layers'] = str(layer_id[layer['bottom']] -\n",
    "                                          len(blocks))\n",
    "                    blocks.append(block)\n",
    "                block = OrderedDict()\n",
    "                block['type'] = layer['type']\n",
    "                top = layer['top']\n",
    "                layer_id[top] = len(blocks)\n",
    "                blocks.append(block)\n",
    "\n",
    "                i = i + 1\n",
    "\n",
    "        print('done')\n",
    "        assert self.out_file is not None\n",
    "        weightfile = self.out_file + '.weights'\n",
    "        cfgfile = self.out_file + '.cfg'\n",
    "        _save_weights(np.array(wdata), weightfile)\n",
    "        _save_cfg(blocks, cfgfile)\n",
    "        _print_cfg(blocks)\n",
    "        _print_cfg_nicely(blocks)\n",
    "        print('Hash of Darknet model has been published: ',\n",
    "              format(_generate_hash(weightfile, cfgfile)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a71330c-0e35-4663-8ee9-0b46d369fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     c2d = Caffe2Darknet(net='./ResNet18/resnet18.prototxt',\n",
    "#                         weight='./ResNet18/resnet18.caffemodel',\n",
    "#                         out_file='./ResNet18/resnet18')\n",
    "#     c2d.convert()\n",
    "#     _check_hash('./ResNet18/resnet18.weights', './ResNet18/resnet18.cfg',\n",
    "#                 'fc7ccc2dc0e8966994cc28ac0841cddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece97b00-0c73-4f96-8c01-b850970ccff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caffe_0618",
   "language": "python",
   "name": "caffe_0618"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
